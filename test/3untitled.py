# -*- coding: utf-8 -*-
"""3Untitled.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZXx6Ebu0b33qPuYYX_tCDzaMR0pgi0sq
"""

# Importing libraries for the model
import os
import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from PIL import Image
import matplotlib.pyplot as plt

from google.colab import drive
drive.mount('/content/drive')

# Path to the training datasets
train_dog_path = '/content/drive/MyDrive/train/dogs'  # Use Google Drive path or Colab file path
train_cat_path = '/content/drive/MyDrive/train/cats'

# Function to resize images in a folder
def resize_images_in_folder(folder_path):
    for file in os.listdir(folder_path):
        file_path = os.path.join(folder_path, file)
        img = Image.open(file_path)
        img = img.resize((112, 112))
        img.save(file_path)

# Resize images in training dog and cat folders
resize_images_in_folder(train_dog_path)
resize_images_in_folder(train_cat_path)

# Path to the testing datasets
test_dog_path = '/content/drive/MyDrive/train/dogs'  # Use Google Drive path or Colab file path
test_cat_path = '/content/drive/MyDrive/train/cats'

# Resize images in testing dog and cat folders
resize_images_in_folder(test_dog_path)
resize_images_in_folder(test_cat_path)

!mkdir train
!mkdir test

# Setting up ImageDataGenerators for training and testing datasets
IMAGE_SIZE = 112
BATCH_SIZE = 32
train_data_size = 180
test_data_size = 20

train_datagen = ImageDataGenerator(
    rescale=1./255,
    rotation_range=90,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True
)

test_datagen = ImageDataGenerator(
    rescale=1./255,
    rotation_range=90,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True
)

# Load images into generators
train_generator = train_datagen.flow_from_directory(
    '/content/drive/MyDrive/train',  # Path to training folder
    target_size=(IMAGE_SIZE, IMAGE_SIZE),
    batch_size=BATCH_SIZE,
    class_mode='binary'
)

test_generator = test_datagen.flow_from_directory(
    '/content/drive/MyDrive/train',  # Path to testing folder
    target_size=(IMAGE_SIZE, IMAGE_SIZE),
    batch_size=BATCH_SIZE,
    class_mode='binary'
)

# Building the CNN model
model = Sequential([
    Conv2D(32, (3, 3), activation='relu', input_shape=(IMAGE_SIZE, IMAGE_SIZE, 3)),
    MaxPool2D(2, 2),
    Conv2D(32, (3, 3), activation='relu'),
    MaxPool2D(2, 2),
    Flatten(),
    Dense(100, activation='relu'),
    Dense(1, activation='sigmoid')  # Sigmoid for binary classification
])

# Summarize the model
model.summary()

# Compile the model with Adam optimizer and binary crossentropy loss function
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Train the model
history = model.fit(
    train_generator,
    epochs=10,
    validation_data=test_generator
)

# Plotting training and validation accuracy/loss
plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Model Performance')
plt.xlabel('Epochs')
plt.ylabel('Accuracy/Loss')
plt.legend()
plt.show()